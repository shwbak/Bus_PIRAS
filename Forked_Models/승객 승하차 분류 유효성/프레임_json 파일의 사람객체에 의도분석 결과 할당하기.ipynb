{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPOWwdLvpTA9+fgqUq1N/aA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gVaZXA3BQOnp"},"outputs":[],"source":["from __future__ import print_function\n","from __future__ import absolute_import\n","from __future__ import division\n","\n","from glob import glob\n","import os\n","from tqdm import tqdm\n","from functools import cmp_to_key\n","import json\n","import random\n","import math\n","import numpy as np\n","from datetime import datetime\n","from pytz import timezone\n","import shutil\n","from tensorflow import keras\n","import logging\n","import traceback\n","logging.basicConfig(level=logging.ERROR)"]},{"cell_type":"code","source":["%cd \"/content/drive/Othercomputers/내 노트북/Forked_Models/승객 승하차 분류 유효성/Model 추가 학습/데이터&추가학습된 모델&유틸\"\n","\n","from __future__ import print_function\n","from __future__ import absolute_import\n","\n","######################################\n","# Import Library\n","######################################\n","import tensorflow as tf\n","from tensorflow import keras\n","from datetime import datetime\n","import os\n","from pytz import timezone\n","import numpy as np\n","from utils.dev_env_info import getDevEnvInfo\n","from utils.data import GetOnOffDataset\n","from glob import glob\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","######################################\n","# Set GPU\n","######################################\n","os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n","\n","\n","######################################\n","# Set GPU Memory\n","######################################\n","gpus = tf.config.experimental.list_physical_devices('GPU')\n","print(gpus)\n","# if gpus:\n","#   try:\n","#     tf.config.experimental.set_virtual_device_configuration(\n","#         gpus[0],\n","#         [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10240)])\n","#   except RuntimeError as e:\n","#     print(e)\n","\n","\n","######################################\n","# Set Functions\n","######################################\n","def getConfusionMatrixLog(confusion_matrix_array, class_name):\n","    dst = ''\n","    dst += '{0:15s}{1:15s}{2:s}\\n'.format('', '', 'Predicted Class')\n","    for idx in range(len(class_name) + 1):\n","        if idx == 0:\n","            dst += '{0:15s}{1:15s}'.format('', '')\n","            for name in class_name:\n","                dst += '{0:15s}'.format(name)\n","\n","        else:\n","            if idx == 1:\n","                dst += '\\n{0:15s}'.format('True Class')\n","                dst += '{0:15s}'.format(class_name[idx - 1])\n","\n","            else:\n","                dst += '{0:15s}{1:15s}'.format('', class_name[idx - 1])\n","\n","            for value in confusion_matrix_array[idx - 1]:\n","                dst += '{0:15s}'.format(str(value))\n","            dst += '\\n'\n","\n","    return dst\n","\n","\n","def getClassReportLog(class_report_dict, class_name):\n","    dst = ''\n","    dst += '{0:15s}{1:15s}{2:15s}{3:15s}\\n'.format('', 'Precision', 'Recall', 'F1-score')\n","    for name in class_name:\n","        precision = class_report_dict[name]['precision']\n","        precision = '{0:.2f}'.format(precision)\n","        recall = class_report_dict[name]['recall']\n","        recall = '{0:.2f}'.format(recall)\n","        f1_score = class_report_dict[name]['f1-score']\n","        f1_score = '{0:.2f}'.format(f1_score)\n","        dst += '{0:15s}{1:15s}{2:15s}{3:15s}\\n'.format(name,\n","                                                       precision,\n","                                                       recall,\n","                                                       f1_score)\n","\n","    precision = class_report_dict['weighted avg']['precision']\n","    precision = '{0:.2f}'.format(precision)\n","    recall = class_report_dict['weighted avg']['recall']\n","    recall = '{0:.2f}'.format(recall)\n","    f1_score = class_report_dict['weighted avg']['f1-score']\n","    f1_score = '{0:.2f}'.format(f1_score)\n","    dst += '\\n{0:15s}{1:15s}{2:15s}{3:15s}\\n'.format('Total',\n","                                                     precision,\n","                                                     recall,\n","                                                     f1_score)\n","\n","    return dst\n","\n","\n","######################################\n","# Global Parameters\n","######################################\n","fmt = '%Y-%m-%d %H:%M:%S %Z%z'\n","cur_dir = os.getcwd()\n","dataset_dir = os.path.join(cur_dir, \"dataset\")\n","model_save_dir = os.path.join(cur_dir, '')\n","f = open(\"/content/drive/Othercomputers/내 노트북/Forked_Models/승객 승하차 분류 유효성/Model 추가 학습/데이터&추가학습된 모델&유틸/output/evaluate_log.txt\", 'w')\n","class_name = ['Get On', 'Get Off', 'Idle']\n","\n","\n","######################################\n","# Get Development Environment Info\n","######################################\n","cur_time = datetime.now(timezone('Asia/Seoul')).strftime(fmt)\n","timestamp_str = '{0:s}'.format('[Timestamp: ' + cur_time + ']')\n","tmp_str = timestamp_str + ' 개발 환경 정보'\n","log_str = tmp_str + '\\n'\n","f.write(log_str)\n","print(tmp_str)\n","\n","tmp_str = getDevEnvInfo()\n","log_str = tmp_str + '\\n'\n","f.write(log_str)\n","print(tmp_str)\n"],"metadata":{"id":"fSpzbkbESmkv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class GetOnOffData:                  #data.py에서 가져온 class\n","    def __init__(self, original_data_dir, get_on_off_data_dir, dataset_dir, train_val_rate):\n","        self.original_data_list = self.__getOriginalDataList(original_data_dir)\n","        self.original_data_dir = original_data_dir\n","        self.get_on_off_data_dir = get_on_off_data_dir\n","        self.train_val_rate = train_val_rate\n","        self.original_num = 0\n","        self.get_on_json_list = []\n","        self.get_off_json_list = []\n","        self.idle_json_list = []\n","        self.get_on_num = 0\n","        self.get_off_num = 0\n","        self.idle_num = 0\n","        self.dataset_dir = dataset_dir\n","        self.train_get_on_num = 0\n","        self.val_get_on_num = 0\n","        self.test_get_on_num = 0\n","        self.train_get_off_num = 0\n","        self.val_get_off_num = 0\n","        self.test_get_off_num = 0\n","        self.train_idle_num = 0\n","        self.val_idle_num = 0\n","        self.test_idle_num = 0\n","        self.train_num = 0\n","        self.val_num = 0\n","        self.test_num = 0\n","        self.train_list = []\n","        self.val_list = []\n","        self.test_list = []\n","\n","   def makeGetOnOffData_getPrediction(self):\n","        # Get Model(AIhub에서 제공한 모델 원본)\n","        loaded_model = keras.models.load_model(\"/content/drive/Othercomputers/내 노트북/Forked_Models/승객 승하차 분류 유효성/Model 추가 학습/fs_lstm_get_on_off_chC.h5\")\n","\n","        # Make dir\n","        get_on_dir = os.path.join(self.get_on_off_data_dir, 'get_on')\n","        get_off_dir = os.path.join(self.get_on_off_data_dir, 'get_off')\n","        idle_dir = os.path.join(self.get_on_off_data_dir, 'idle')\n","        os.makedirs(get_on_dir, exist_ok=True)\n","        os.makedirs(get_off_dir, exist_ok=True)\n","        os.makedirs(idle_dir, exist_ok=True)\n","\n","        # Count Parameters\n","        get_on_count = 0\n","        get_off_count = 0\n","        idle_count = 0\n","\n","        # file name parameters\n","        get_on_data_name = 'get_on_'\n","        get_off_data_name = 'get_off_'\n","        idle_data_name = 'idle_'\n","\n","        # Make get on off data\n","        for json_list in tqdm(self.original_data_list):\n","            # Get Passenger Data\n","            for i in range(len(json_list) - 2):\n","                # Get Json data\n","                json_path_1 = os.path.join(json_list[i])\n","                json_path_2 = os.path.join(json_list[i + 1])\n","                json_path_3 = os.path.join(json_list[i + 2])\n","                try:\n","                    with open(json_path_1, 'r') as json_file:\n","                        json_data_1 = json.load(json_file)\n","                except Exception:\n","                    raise Exception('json_path_1: ', json_path_1)\n","                try:\n","                    with open(json_path_2, 'r') as json_file:\n","                        json_data_2 = json.load(json_file)\n","                except Exception:\n","                    raise Exception('json_path_2: ', json_path_2)\n","                try:\n","                    with open(json_path_3, 'r') as json_file:\n","                        json_data_3 = json.load(json_file)\n","                except Exception:\n","                    raise Exception('json_path_3: ', json_path_3)\n","\n","                # Get Img width & height\n","                width = json_data_1['info']['width']\n","                height = json_data_1['info']['height']\n","\n","                # Get Valid data id and Append data\n","                if len(json_data_1['annotations']) != 0 and len(json_data_2['annotations']) != 0 and len(\n","                        json_data_3['annotations']) != 0:\n","                    for annotation_1 in json_data_1['annotations']:\n","                        id_1 = annotation_1['id']\n","                        for annotation_2 in json_data_2['annotations']:\n","                            id_2 = annotation_2['id']\n","                            if id_1 != id_2:\n","                                continue\n","                            else:\n","                                for annotation_3 in json_data_3['annotations']:\n","                                    id_3 = annotation_3['id']\n","                                    if id_2 != id_3:\n","                                        continue\n","                                    else:\n","                                        # Checking neck keypoint is exist\n","                                        if annotation_1['keypoints'][26] == 0 and annotation_2['keypoints'][26] == 0 and \\\n","                                                annotation_3['keypoints'][26] == 0:\n","                                            break\n","                                        # Checking head keypoint is exist\n","                                        elif annotation_1['keypoints'][29] == 0 and annotation_2['keypoints'][\n","                                            29] == 0 and annotation_3['keypoints'][29] == 0:\n","                                            break\n","                                        # Checking right shoulder keypoint is exist\n","                                        elif annotation_1['keypoints'][38] == 0 and annotation_2['keypoints'][\n","                                            38] == 0 and annotation_3['keypoints'][38] == 0:\n","                                            break\n","                                        # Checking left shoulder keypoint is exist\n","                                        elif annotation_1['keypoints'][41] == 0 and annotation_2['keypoints'][\n","                                            41] == 0 and annotation_3['keypoints'][41] == 0:\n","                                            break\n","\n","                                        # Get Data of neck keypoint\n","                                        neck_x_1 = annotation_1['keypoints'][24] / width\n","                                        neck_y_1 = annotation_1['keypoints'][25] / height\n","                                        neck_x_2 = annotation_2['keypoints'][24] / width\n","                                        neck_y_2 = annotation_2['keypoints'][25] / height\n","                                        neck_x_3 = annotation_3['keypoints'][24] / width\n","                                        neck_y_3 = annotation_3['keypoints'][25] / height\n","\n","                                        # Get Data of head keypoint\n","                                        head_x_1 = annotation_1['keypoints'][27] / width\n","                                        head_y_1 = annotation_1['keypoints'][28] / height\n","                                        head_x_2 = annotation_2['keypoints'][27] / width\n","                                        head_y_2 = annotation_2['keypoints'][28] / height\n","                                        head_x_3 = annotation_3['keypoints'][27] / width\n","                                        head_y_3 = annotation_3['keypoints'][28] / height\n","\n","                                        # Get Data of right shoulder keypoint\n","                                        r_shoulder_x_1 = annotation_1['keypoints'][36] / width\n","                                        r_shoulder_y_1 = annotation_1['keypoints'][37] / height\n","                                        r_shoulder_x_2 = annotation_2['keypoints'][36] / width\n","                                        r_shoulder_y_2 = annotation_2['keypoints'][37] / height\n","                                        r_shoulder_x_3 = annotation_3['keypoints'][36] / width\n","                                        r_shoulder_y_3 = annotation_3['keypoints'][37] / height\n","\n","                                        # Get Data of left shoulder keypoint\n","                                        l_shoulder_x_1 = annotation_1['keypoints'][39] / width\n","                                        l_shoulder_y_1 = annotation_1['keypoints'][40] / height\n","                                        l_shoulder_x_2 = annotation_2['keypoints'][39] / width\n","                                        l_shoulder_y_2 = annotation_2['keypoints'][40] / height\n","                                        l_shoulder_x_3 = annotation_3['keypoints'][39] / width\n","                                        l_shoulder_y_3 = annotation_3['keypoints'][40] / height\n","\n","                                        # Get Vector of neck->rShoulder\n","                                        vector_r_shoulder_x_1 = r_shoulder_x_1 - neck_x_1\n","                                        vector_r_shoulder_y_1 = r_shoulder_y_1 - neck_y_1\n","                                        vector_r_shoulder_x_2 = r_shoulder_x_2 - neck_x_2\n","                                        vector_r_shoulder_y_2 = r_shoulder_y_2 - neck_y_2\n","                                        vector_r_shoulder_x_3 = r_shoulder_x_3 - neck_x_3\n","                                        vector_r_shoulder_y_3 = r_shoulder_y_3 - neck_y_3\n","\n","                                        # Get Vector of neck->lShoulder\n","                                        vector_l_shoulder_x_1 = l_shoulder_x_1 - neck_x_1\n","                                        vector_l_shoulder_y_1 = l_shoulder_y_1 - neck_y_1\n","                                        vector_l_shoulder_x_2 = l_shoulder_x_2 - neck_x_2\n","                                        vector_l_shoulder_y_2 = l_shoulder_y_2 - neck_y_2\n","                                        vector_l_shoulder_x_3 = l_shoulder_x_3 - neck_x_3\n","                                        vector_l_shoulder_y_3 = l_shoulder_y_3 - neck_y_3\n","\n","                                        # Get Vector of neck->head\n","                                        vector_head_x_1 = head_x_1 - neck_x_1\n","                                        vector_head_y_1 = head_y_1 - neck_y_1\n","                                        vector_head_x_2 = head_x_2 - neck_x_2\n","                                        vector_head_y_2 = head_y_2 - neck_y_2\n","                                        vector_head_x_3 = head_x_3 - neck_x_3\n","                                        vector_head_y_3 = head_y_3 - neck_y_3\n","\n","                                        # Push Data\n","                                        data = [[0 for col in range(8)] for row in range(3)]\n","                                        data[0] = [neck_x_1, neck_y_1,\n","                                                   vector_head_x_1, vector_head_y_1,\n","                                                   vector_r_shoulder_x_1, vector_r_shoulder_y_1,\n","                                                   vector_l_shoulder_x_1, vector_l_shoulder_y_1]\n","                                        data[1] = [neck_x_2, neck_y_2,\n","                                                   vector_head_x_2, vector_head_y_2,\n","                                                   vector_r_shoulder_x_2, vector_r_shoulder_y_2,\n","                                                   vector_l_shoulder_x_2, vector_l_shoulder_y_2]\n","                                        data[2] = [neck_x_3, neck_y_3,\n","                                                   vector_head_x_3, vector_head_y_3,\n","                                                   vector_r_shoulder_x_3, vector_r_shoulder_y_3,\n","                                                   vector_l_shoulder_x_3, vector_l_shoulder_y_3]\n","\n","                                        data_dict = dict()\n","                                        data_dict['data'] = data\n","                                        data_dict['original_data'] = []\n","                                        for i in range(3):\n","                                            tmp_dict = dict()\n","                                            tmp_dict['time_step'] = i + 1\n","                                            if i == 0:\n","                                                tmp_dict['original_data_path'] = json_path_1\n","                                            elif i == 1:\n","                                                tmp_dict['original_data_path'] = json_path_2\n","                                            else:\n","                                                tmp_dict['original_data_path'] = json_path_3\n","                                            tmp_dict['id'] = annotation_3['id']\n","\n","                                            data_dict['original_data'].append(tmp_dict)\n","\n","\n","                                        # Count num of get_on get_off idle\n","                                        if annotation_3['get_on']:\n","                                            get_on_count += 1\n","                                            data_dict['label'] = 0\n","                                            json_file_name = get_on_data_name + str(get_on_count) + '.json'\n","                                            json_file_path = os.path.join(get_on_dir, json_file_name)\n","                                            if not os.path.exists(json_file_path):\n","                                                with open(json_file_path, 'w') as f:\n","                                                    json.dump(data_dict, f, indent=4)\n","\n","                                        elif annotation_3['get_off']:\n","                                            get_off_count += 1\n","                                            data_dict['label'] = 1\n","                                            json_file_name = get_off_data_name + str(get_off_count) + '.json'\n","                                            json_file_path = os.path.join(get_off_dir, json_file_name)\n","                                            if not os.path.exists(json_file_path):\n","                                                with open(json_file_path, 'w') as f:\n","                                                    json.dump(data_dict, f, indent=4)\n","\n","                                        else:\n","                                            idle_count += 1\n","                                            data_dict['label'] = 2\n","                                            json_file_name = idle_data_name + str(idle_count) + '.json'\n","                                            json_file_path = os.path.join(idle_dir, json_file_name)\n","                                            if not os.path.exists(json_file_path):\n","                                                with open(json_file_path, 'w') as f:\n","                                                    json.dump(data_dict, f, indent=4)\n","\n","                                        break\n","\n","                                        # 의도 분석 모델의 예측 결과 얻기\n","                                        with open(json_file_path, 'r') as f:  # 1)생성한 input json 파일에서 모델 입력값 얻어오기\n","                                            json_data = json.load(f)\n","                                            \n","                                        input_data = json_data['data']\n","                                        prediction = loaded_model(input_data)   # 2)모델 예측 결과 얻기\n","                                        \n","                                        # 3)json파일의 사람객체에 의도분석 결과 할당하기\n","                                        if prediction = keras.utils.to_categorical(0, num_classes=3):\n","                                          with open(json_path_3, 'r') as file:\n","                                              temp_json = json.load(file)\n","                                              temp_json[\"annotations\"][id_3][\"get_on\"] = True\n","                                              temp_json[\"annotations\"][id_3][\"get_off\"] = False\n","                                          with open(json_path_3, 'w') as file:\n","                                              json.dump(temp_json, file, indent=4)\n","\n","                                        elif prediction = keras.utils.to_categorical(1, num_classes=3):\n","                                          with open(json_path_3, 'r') as file:\n","                                              temp_json = json.load(file)\n","                                              temp_json[\"annotations\"][id_3][\"get_off\"] = True\n","                                              temp_json[\"annotations\"][id_3][\"get_on\"] = False\n","                                          with open(json_path_3, 'w') as file:\n","                                              json.dump(temp_json, file, indent=4)\n","                                        \n","                                        else:\n","                                          with open(json_path_3, 'r') as file:\n","                                              temp_json = json.load(file)\n","                                              temp_json[\"annotations\"][id_3][\"get_on\"] = False\n","                                              temp_json[\"annotations\"][id_3][\"get_off\"] = False\n","                                          with open(json_path_3, 'w') as file:\n","                                              json.dump(temp_json, file, indent=4)\n","\n","                            break\n","\n","        self.get_on_num = get_on_count\n","        self.get_off_num = get_off_count\n","        self.idle_num = idle_count"],"metadata":{"id":"xvnt94myQXTO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["######################################\n","# Get Test Data\n","######################################\n","# Start make test numpy array\n","cur_time = datetime.now(timezone('Asia/Seoul')).strftime(fmt)\n","timestamp_str = '{0:s}'.format('[Timestamp: ' + cur_time + ']')\n","tmp_str = timestamp_str + ' Test 데이터 정보 가져오기 시작'\n","log_str = tmp_str + '\\n'\n","f.write(log_str)\n","print(tmp_str)\n","\n","get_on_off_dataset = GetOnOffDataset(dataset_dir)\n","test_list = get_on_off_dataset.test_list\n","\n","# Get test data list\n","cur_time = datetime.now(timezone('Asia/Seoul')).strftime(fmt)\n","timestamp_str = '{0:s}'.format('[Timestamp: ' + cur_time + ']')\n","tmp_str = timestamp_str + ' Test 데이터 목록'\n","log_str = tmp_str + '\\n'\n","f.write(log_str)\n","print(tmp_str)\n","\n","tmp_str = ''\n","for idx, test_json_path in enumerate(test_list):\n","    tmp_str += str(idx + 1) + '. ' + test_json_path + '\\n'\n","    print(test_json_path)\n","tmp_str += '\\n'\n","f.write(tmp_str)\n","\n","# Get test numpy array\n","cur_time = datetime.now(timezone('Asia/Seoul')).strftime(fmt)\n","timestamp_str = '{0:s}'.format('[Timestamp: ' + cur_time + ']')\n","tmp_str = timestamp_str + ' Test 데이터 numpy 배열 정보'\n","log_str = tmp_str + '\\n'\n","f.write(log_str)\n","print(tmp_str)\n","test_x, test_y = get_on_off_dataset.getTestDatasetNumpyArray()\n","log_str = ''\n","log_str += 'Test_X Shape: {}\\n'.format(test_x.shape)\n","log_str += 'Test_Y Shape: {}\\n'.format(test_y.shape)\n","log_str += '\\n'\n","f.write(log_str)\n","print(log_str)\n","\n","# Get Test Dataset API\n","test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n","test_dataset = test_dataset.batch(1)\n","\n","######################################\n","# Start Evaluate\n","######################################\n","cur_time = datetime.now(timezone('Asia/Seoul')).strftime(fmt)\n","timestamp_str = '{0:s}'.format('[Timestamp: ' + cur_time + ']')\n","tmp_str = timestamp_str + ' 모델 평가 시작'\n","log_str = tmp_str + '\\n'\n","f.write(log_str)\n","print(tmp_str)\n","\n","y_true = []\n","y_pred = []\n","for idx, (input_x, input_y) in enumerate(test_dataset.as_numpy_iterator()):\n","    label = np.argmax(input_y, axis=1)\n","    prediction = np.argmax(model(input_x, training=False), axis=1)\n","\n","    cur_time = datetime.now(timezone('Asia/Seoul')).strftime(fmt)\n","    timestamp_str = '{0:s}'.format('[Timestamp: ' + cur_time + ']')\n","    tmp_str = timestamp_str + ' ' + str(idx+1) + '. ' + test_list[idx] + ' Test 데이터 인풋 결과'\n","    log_str = tmp_str + '\\n'\n","    f.write(log_str)\n","    print(tmp_str)\n","\n","    tmp_str = ''\n","    label_name = ''\n","    pred_name = ''\n","\n","    if label == 0:\n","        label_name = 'Get On'\n","    elif label == 1:\n","        label_name = 'Get Off'\n","    else:\n","        label_name = 'Idle'\n","\n","    if prediction == 0:\n","        pred_name = 'Get On'\n","    elif prediction == 1:\n","        pred_name = 'Get Off'\n","    else:\n","        pred_name = 'Idle'\n","\n","    tmp_str += '정답: {}\\n'.format(label_name)\n","    tmp_str += '예측: {}\\n'.format(pred_name)\n","    f.write(tmp_str)\n","    print(tmp_str)\n","\n","    y_true += list(label)\n","    y_pred += list(prediction)"],"metadata":{"id":"yTLJ1TA2QPoh"},"execution_count":null,"outputs":[]}]}